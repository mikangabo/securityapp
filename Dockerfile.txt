# Use CentOS as base image
FROM centos:latest

# Install required packages
RUN yum -y update && \
    yum -y install java-11-openjdk wget curl tar && \
    yum clean all

# Set environment variables for Spark and Hadoop
ENV SPARK_VERSION=3.3.2
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH="$SPARK_HOME/bin:$PATH"

# Download and install Apache Spark
RUN wget https://downloads.apache.org/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz && \
    tar -xvzf spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz && \
    mv spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION /opt/spark && \
    rm spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz

# Set working directory
WORKDIR /app

# Copy application files
COPY refactored.py /app/refactored.py

# Install Python and dependencies
RUN yum -y install python3 python3-pip && \
    pip3 install pyspark pandas

# Run the Python script
CMD ["python3", "/app/refactored.py"]
